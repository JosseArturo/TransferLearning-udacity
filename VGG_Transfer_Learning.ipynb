{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_Transfer_Learning.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWz8DNrLJAMh",
        "colab_type": "text"
      },
      "source": [
        "# Demo: Using VGG with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nyi4qqpJAMj",
        "colab_type": "text"
      },
      "source": [
        "Below, you'll be able to check out the predictions from an ImageNet pre-trained VGG network with Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfeovzz0JAMl",
        "colab_type": "text"
      },
      "source": [
        "### Load some example images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B90igUIEJAMm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "7dd9b17c-9818-48b4-931b-620b02a47246"
      },
      "source": [
        "# Load our images first, and we'll check what we have\n",
        "from glob import glob\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_paths = glob('images/*.jpg')\n",
        "\n",
        "# Print out the image paths\n",
        "print(image_paths)\n",
        "\n",
        "# View an example of an image\n",
        "example = mpimg.imread(image_paths[0])\n",
        "plt.imshow(example)\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fabd4a3be919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# View an example of an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N0Jm4nIJAMu",
        "colab_type": "text"
      },
      "source": [
        "### Pre-process an image\n",
        "Note that the `image.load_img()` function will re-size our image to 224x224 as desired for input into this VGG16 model, so the images themselves don't have to be 224x224 to start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beSSqu0OJAMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here, we'll load an image and pre-process it\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "i = 0 # Can change this to your desired image to test\n",
        "img_path = image_paths[i]\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6sB6z5LJAM0",
        "colab_type": "text"
      },
      "source": [
        "### Load VGG16 pre-trained model\n",
        "We won't throw out the top fully-connected layer this time when we load the model, as we actually want the true ImageNet-related output. However, you'll learn how to do this in a later lab. The inference will be a little slower than you might expect here as we are not using GPU just yet.\n",
        "\n",
        "Note also the use of `decode_predictions` which will map the prediction to the class name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMYjayLUJAM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note - this will likely need to download a new version of VGG16\n",
        "from keras.applications.vgg16 import VGG16, decode_predictions\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = VGG16(weights='imagenet')\n",
        "\n",
        "# Perform inference on our pre-processed image\n",
        "predictions = model.predict(x)\n",
        "\n",
        "# Check the top 3 predictions of the model\n",
        "print('Predicted:', decode_predictions(predictions, top=3)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNqc2-kVJAM8",
        "colab_type": "text"
      },
      "source": [
        "You should mostly get the correct answers here. In our own run, it predicted a Tusker elephant with an African elephant in second place (the image is of an African elephant), correctly selected a labrador, and very confidently predicted a zebra. You can add some of your own images into the `images/` folder by clicking on the jupyter logo in the top left and see how it performs on your own examples!"
      ]
    }
  ]
}